# DevOps Copilot - Automated Incident Management & Workflow Orchestration

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)
![Celery](https://img.shields.io/badge/Celery-5.3-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![Redis](https://img.shields.io/badge/Redis-7-red.svg)

An intelligent DevOps assistant that automates incident response, postmortem generation, and knowledge base management through orchestrated workflows powered by AI.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Problems Solved](#problems-solved)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Directory Structure](#directory-structure)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Deployment](#deployment)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Workflows](#workflows)
- [Development](#development)
- [Contributing](#contributing  )

---

## ğŸ¯ Overview

DevOps Copilot is an AI-powered incident management system that automates  incident lifecycleâ€”from initial response to postmortem documentation. Built with modern async workflows using Celery, it integrates with GitHub, ChromaDB for semantic search, and uses Claude AI for intelligent analysis and documentation generation.

### Built With

- **Backend**: Python 3.11, FastAPI (REST API)
- **Task Queue**: Celery with Redis broker
- **Databases**: PostgreSQL (relational), ChromaDB (vector embeddings)
- **AI/LLM**: Anthropic Claude API
- **Orchestration**: Docker Compose
- **Integrations**: GitHub API, Jinja2 templating

---

## ğŸ”§ Problems Solved

### 1. **Manual Incident Response Overhead** âŒ â†’ âœ…
**Problem**: DevOps engineers spend valuable time during incidents performing repetitive tasks: analyzing logs, searching for runbooks, creating tracking issues, and sending notifications.

**Solution**: Automated incident response workflow that executes all these steps in parallel/sequence, reducing response time from hours to minutes.

### 2. **Inconsistent Postmortem Documentation** âŒ â†’ âœ…
**Problem**: Postmortem quality varies significantly; engineers delay or skip documentation due to time constraints, losing valuable lessons learned.

**Solution**: Automated postmortem generation using AI to analyze incidents and generate structured documentation with consistent quality, automatically indexed for future reference.



**Solution**: Real-time workflow tracking with step-by-step progress monitoring, retry mechanisms with exponential backoff, and comprehensive logging with correlation IDs.

### 5. **Race Conditions in Sync Operations** âŒ â†’ âœ…
**Problem**: Multiple concurrent knowledge base syncs can corrupt indexes or waste resources processing the same files.

**Solution**: Distributed locking mechanism (Redis-based) ensures only one sync operation runs at a time, with clear conflict messaging (HTTP 409).

---

## âœ¨ Key Features

### ğŸš¨ Incident Response Automation
- **Automatic Workflow Trigger**: Creates workflows when incidents are reported
- **Log Analysis**: Asynchronous log parsing and error extraction
- **GitHub Integration**: Auto-creates tracking issues with incident details
- **Retry Logic**: Exponential backoff (1s, 2s, 4s) with max 3 retries per ste

### ğŸ“„ Postmortem Publishing
- **AI-Powered Generation**: Uses Claude to create incident summaries, timelines, root cause analysis
- **Template Rendering**: Jinja2 templates for consistent documentation format
- **Parallel Processing**: Creates GitHub issues and ChromaDB embeddings simultaneously
- **Stakeholder Notifications**: Automatic distribution to configured recipients
- **Searchable Archives**: All postmortems indexed for future incident reference


### ğŸ”„ Workflow Orchestration
- **Celery Chains**: Sequential task execution with result passing
- **Celery Groups**: Parallel task execution for independent operations
- **Celery Chords**: Wait for parallel tasks before proceeding (embeddings â†’ ChromaDB)
- **State Persistence**: Workflows survive system restarts (database-backed)
- **Progress Tracking**: Real-time status via REST API with cache layer (Redis)

### ğŸ¯ Developer Experience
- **OpenAPI Documentation**: Auto-generated interactive API docs at `/docs`
- **Health Checks**: Service readiness endpoints for all components
- **Structured Logging**: JSON logs with correlation IDs for request tracing
- **Docker Compose**: One-command local development environment


---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Client Applications                       â”‚
â”‚                     (Web UI, CLI, API clients)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP REST
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Backend (Port 8000)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Incident   â”‚  â”‚  Postmortem  â”‚  â”‚   KB Sync Routes    â”‚  â”‚
â”‚  â”‚   Routes     â”‚  â”‚    Routes    â”‚  â”‚                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                      â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           Workflow Service & Cache Layer                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                      â”‚
          â–¼                  â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Celery Task Queue                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Incident Tasks  â”‚ â”‚ Postmortem Tasksâ”‚ â”‚   â”‚ â”‚
â”‚  â”‚ â€¢ create_record â”‚ â”‚ â€¢ generate_sectionsâ”‚ â€¢  â”‚ â”‚
â”‚  â”‚ â€¢ analyze_logs  â”‚ â”‚ â€¢ render_template â”‚ â”‚ â€¢  â”‚ â”‚
â”‚  â”‚ â€¢ search_runbooksâ”‚ â”‚ â€¢ embed_chromadb â”‚ â”‚ â€¢  â”‚
â”‚  â”‚ â€¢ create_issue  â”‚ â”‚ â€¢ notify_stakeholdersâ”‚ â€¢  â”‚
â”‚  â”‚ â€¢ send_notify   â”‚ â”‚                  â”‚ â”‚ â€¢  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                      â”‚
         â–¼                  â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL   â”‚  â”‚    Redis     â”‚  â”‚       ChromaDB           â”‚
â”‚  (Workflows,   â”‚  â”‚ (Broker +    â”‚  â”‚  (Vector Embeddings)     â”‚
â”‚   Incidents)   â”‚  â”‚  Cache +     â”‚  â”‚  â€¢ Runbooks Collection   â”‚
â”‚                â”‚  â”‚  Locks)      â”‚  â”‚  â€¢ Postmortems Collectionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         External Integrations
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  GitHub API  â”‚  â”‚  Claude API  â”‚
         â”‚ (Issues)     â”‚  â”‚ (AI Analysis)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Execution Flow

**Incident Response Workflow**:
```
1. POST /api/workflows/incident/{id}
   â†“
2. Create Workflow DB record
   â†“
3. Celery Chain:
   create_incident_record â†’ analyze_logs_async â†’ search_related_runbooks
   â†’ create_github_issue â†’ send_notification
   â†“
4. Each step updates workflow state in DB + Redis cache
   â†“
5. GET /api/workflows/{workflow_id} returns progress
```

**KB Sync Workflow**:
```
1. POST /api/workflows/kb-sync (acquires distributed lock)
   â†“
2. Celery Chain:
   scan_runbooks_dir â†’ extract_and_process_changes
   â†“
3. Parallel Group (for each changed file):
   regenerate_embeddings_1 â”‚ regenerate_embeddings_2 â”‚ ... â”‚ regenerate_embeddings_N
   â†“
4. Chord Callback (waits for all embeddings):
   prepare_chromadb_update â†’ invalidate_cache
   â†“
5. Lock automatically released (timeout: 10 minutes)
```

---

## ğŸ“ Directory Structure

```
ia-dev-final-project/
â”œâ”€â”€ backend/                      # Main application code
â”‚   â”œâ”€â”€ api/                      # FastAPI routes
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ workflows.py      # Workflow API endpoints
â”‚   â”œâ”€â”€ models/                   # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ base.py              # Base model class
â”‚   â”‚   â”œâ”€â”€ workflow.py          # Workflow & WorkflowStep models
â”‚   â”‚   â”œâ”€â”€ incident.py          # Incident model
â”‚   â”‚   â””â”€â”€ cluster.py           # ClusterConfig model
â”‚   â”œâ”€â”€ services/                # Business logic layer
â”‚   â”‚   â”œâ”€â”€ workflow_service.py  # Workflow CRUD operations
â”‚   â”‚   â”œâ”€â”€ workflow_cache.py    # Redis caching & locking
â”‚   â”‚   â”œâ”€â”€ sync_service.py      # KB sync change detection
â”‚   â”‚   â”œâ”€â”€ embedding_service.py # ChromaDB operations
â”‚   â”‚   â””â”€â”€ notification_service.py # Webhook notifications
â”‚   â”œâ”€â”€ workflows/               # Celery workflow compositions
â”‚   â”‚   â”œâ”€â”€ incident_response.py # Incident workflow chain
â”‚   â”‚   â”œâ”€â”€ postmortem_publish.py# Postmortem workflow chain
â”‚   â”‚   
â”‚   â”‚   â””â”€â”€ tasks/               # Individual Celery tasks
â”‚   â”‚       â”œâ”€â”€ incident_tasks.py
â”‚   â”‚       â”œâ”€â”€ postmortem_tasks.py
â”‚   â”‚       â””â”€â”€ kb_sync_tasks.py
â”‚   â”œâ”€â”€ integrations/            # External API clients
â”‚   â”‚   â”œâ”€â”€ github_client.py     # GitHub API wrapper
â”‚   â”‚   â””â”€â”€ claude_client.py     # Anthropic Claude client
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”‚   â”œâ”€â”€ logging.py           # Structured logging
â”‚   â”‚   â”œâ”€â”€ retry.py             # Retry decorator
â”‚   â”‚   â”œâ”€â”€ file_scanner.py      # Directory scanning
â”‚   â”‚   â””â”€â”€ log_parser.py        # Log analysis
â”‚   â”œâ”€â”€ config/                  # Configuration
â”‚   â”‚   â””â”€â”€ celery_config.py     # Celery settings
â”‚   â”œâ”€â”€ templates/               # Jinja2 templates
â”‚   â”‚   â””â”€â”€ postmortem.md.j2     # Postmortem template
â”‚   â”œâ”€â”€ alembic/                 # Database migrations
â”‚   â”‚   â””â”€â”€ versions/            # Migration scripts
â”‚   â”œâ”€â”€ celery_app.py            # Celery application setup
â”‚   â”œâ”€â”€ database.py              # Database connection
â”‚   â”œâ”€â”€ main.py                  # FastAPI application
â”‚   â””â”€â”€ alembic.ini              # Alembic configuration
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â””â”€â”€ contract/                # Contract tests
â”œâ”€â”€ docker-compose.yml           # Docker Compose configuration
â”œâ”€â”€ Dockerfile.backend           # Backend service Dockerfile
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Environment variables (not in git)
â”œâ”€â”€ Test-evidences.docx          # Test evidences
â”œâ”€â”€ logs-examples/               # Log examples for testing
â””â”€â”€ README.md                    # This file
```

---

## ğŸ“‹ Prerequisites

### Required Software

- **Docker**: 20.10+ and Docker Compose 2.0+
- **Python**: 3.11+ (for local development)
- **Git**: For version control



### External Services

- **Anthropic API Key**: For Claude AI integration
  - Sign up at https://console.anthropic.com/
  - Get API key from dashboard

- **GitHub Personal Access Token**: For issue creation
  - Settings â†’ Developer settings â†’ Personal access tokens
  - Scopes needed: `repo` (full control of private repositories)

---

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/ia-dev-final-project.git
cd ia-dev-final-project
```

### 2. Configure Environment Variables

Edit docker-compose.yml and fill in the environment variables:
GITHUB_TOKEN=YOUR_GITHUB_TOKEN
ANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY
GITHUB_REPO=YOUR_GITHUB_REPO
GITHUB_ENABLED=True

The rest of the environment variables are optional and are already set to default values.

### 3. Build and Run the Application

```bash
docker-compose up --build
```

### 4. Access the Application

Once the containers are up and running, you can access the application at:

- **API Documentation**: http://localhost:8000/docs
- **Celery Dashboard**: http://localhost:5555

### 5. Stop the Application

To stop the application, run:

```bash
docker-compose down
```

### 6. Install Dependencies (Local Development)

```bash
# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Development dependencies
```

---

## âš™ï¸ Configuration

### Database Configuration

PostgreSQL is configured via `DATABASE_URL` environment variable:

```env
DATABASE_URL=postgresql://user:password@host:port/database_name
```

### Celery Configuration

Celery settings in `backend/config/celery_config.py`:

```python
broker_url = "redis://redis:6379/1"
result_backend = "redis://redis:6379/2"
task_serializer = "json"
result_serializer = "json"
task_acks_late = True
worker_prefetch_multiplier = 1
```

### ChromaDB Configuration

Vector database connection settings:

```env
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000
```

Collections created automatically:
- `runbooks` - Runbook documentation embeddings
- `postmortems` - Postmortem document embeddings

---

## ğŸ³ Deployment

### Docker Compose (Recommended for Development)

#### Start All Services

```bash
docker-compose up -d
```

This starts:
- **PostgreSQL** (port 5432) - Main database
- **Redis** (port 6379) - Message broker & cache
- **ChromaDB** (port 8001) - Vector database
- **FastAPI Backend** (port 8000) - REST API
- **Celery Worker** - Background task processor
- **PgAdmin** (port 5050) - Database admin UI

#### Verify Services

```bash
# Check service health
docker-compose ps

# View logs
docker-compose logs -f backend
docker-compose logs -f celery-worker

# Test API
curl http://localhost:8000/docs
```

#### Run Database Migrations

```bash
# Migrations run automatically on backend startup
# To run manually:
docker-compose exec backend alembic upgrade head
```

#### Stop Services

```bash
docker-compose down
# To remove volumes (data):
docker-compose down -v
```

### Kubernetes Deployment (Local with Kind)



#### 1. Build and Load Docker Images

```bash
# Build backend image
docker build -t devops-copilot-backend:latest -f Dockerfile.backend .

```

---

## ğŸ’» Usage

### API Documentation

Interactive API documentation available at:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Quick Start Examples

#### 1. Trigger Incident Response Workflow

```bash
curl -X POST "http://localhost:8000/api/workflows/incident/123e4567-e89b-12d3-a456-426614174000" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "API Service Down",
    "description": "500 errors on /api/chat endpoint",
    "severity": "critical",
    "log_file_path": "/logs/api-2024-01-06.log",
    "triggered_by": "alerting-system"
  }'

# Response:
{
  "workflow_id": "abc123...",
  "type": "INCIDENT_RESPONSE",
  "status": "PENDING",
  "created_at": "2024-01-06T18:30:00Z",
  "message": "Incident response workflow triggered successfully"
}
```

#### 2. Check Workflow Status

```bash
curl "http://localhost:8000/api/workflows/abc123..."

# Response:
{
  "workflow_id": "abc123...",
  "type": "INCIDENT_RESPONSE",
  "status": "RUNNING",
  "progress": "3/5 steps completed",
  "current_step": "create_github_issue",
  "steps": [
    {"name": "create_incident_record", "status": "completed", "order": 1},
    {"name": "analyze_logs_async", "status": "completed", "order": 2},
    {"name": "search_related_runbooks", "status": "completed", "order": 3},
    {"name": "create_github_issue", "status": "running", "order": 4},
    {"name": "send_notification", "status": "pending", "order": 5}
  ]
}
```

#### 3. Trigger Postmortem Generation

```bash
curl -X POST "http://localhost:8000/api/workflows/postmortem/123e4567-e89b-12d3-a456-426614174000"

# Response:
{
  "workflow_id": "def456...",
  "type": "POSTMORTEM_PUBLISH",
  "status": "PENDING",
  "created_at": "2024-01-06T19:00:00Z",
  "message": "Postmortem publish workflow triggered successfully"
}
```


### Using PgAdmin

Access database admin UI:
1. Navigate to http://localhost:5050
2. Login: `admin@admin.com` / `admin123`
3. Add server:
   - Host: `postgres`
   - Port: `5432`
   - Database: `devops_copilot`
   - Username: `user`
   - Password: `pass`

---

## ğŸ“š API Documentation

### Workflow Endpoints

#### `POST /api/workflows/incident/{incident_id}`
Trigger incident response workflow

**Request Body**:
```json
{
  "title": "string",
  "description": "string",
  "severity": "low|medium|high|critical",
  "log_file_path": "string (optional)",
  "triggered_by": "string (optional)"
}
```

**Response**: `202 Accepted` - WorkflowResponse

---

#### `POST /api/workflows/postmortem/{incident_id}`
Trigger postmortem generation workflow

**Response**: `202 Accepted` - WorkflowResponse

---

#### `POST /api/workflows/kb-sync`
Trigger knowledge base synchronization

**Request Body**:
```json
{
  "runbooks_dir": "string",
  "triggered_by": "string (optional)"
}
```

**Response**:
- `202 Accepted` - WorkflowResponse
- `409 Conflict` - If sync already running

---

#### `GET /api/workflows/{workflow_id}`
Get workflow status and progress

**Response**: `200 OK` - WorkflowStatusResponse

---

## ğŸ”„ Workflows

### Incident Response Workflow

**Trigger**: Incident creation via API

**Steps**:
1. **create_incident_record** (0 retries) - Creates database record
2. **analyze_logs_async** (3 retries) - Parses logs for errors
3. **search_related_runbooks** (3 retries) - Semantic search for relevant docs
4. **create_github_issue** (3 retries) - Creates tracking issue


**Composition**: Celery `chain` (sequential execution)

**Retry Strategy**: Exponential backoff with jitter (1s, 2s, 4s)

---

### Postmortem Publishing Workflow

**Trigger**: Manual request for resolved incident

**Steps**:
1. **generate_postmortem_sections** (3 retries) - Claude AI analysis
2. **render_jinja_template** (0 retries) - Renders markdown document
3. **Parallel Execution** (Celery `group`):
   - **create_github_issue** (3 retries)
   - **embed_in_chromadb** (3 retries)


**Composition**: Celery `chain` with `chord` for parallel steps



---

## ğŸ› ï¸ Development

### Local Development Setup

```bash
# Install dependencies
pip install -r requirements.txt -r requirements-dev.txt

# Run migrations
cd backend
alembic upgrade head

# Start FastAPI dev server
uvicorn backend.main:app --reload --port 8000

# Start Celery worker (separate terminal)
celery -A backend.celery_app:app worker --loglevel=info
```

### Code Quality Tools

```bash
# Linting
ruff check .

# Type checking
mypy backend/

# Code formatting
black backend/

# Security scanning
bandit -r backend/
```

### Database Migrations

```bash
# Create new migration
alembic revision --autogenerate -m "Description"

# Apply migrations
alembic upgrade head

# Rollback migration
alembic downgrade -1
```


---

## ğŸ§ª Testing

### Run All Tests

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# Contract tests
pytest tests/contract/ -v

# Coverage report
pytest --cov=backend --cov-report=html
```
### Test Specific Workflows

```bash
# Test incident workflow
pytest tests/integration/test_incident_workflow.py -v

# Test KB sync
pytest tests/integration/test_kb_sync_workflow.py -v
```

---


## ğŸ› Troubleshooting

### Common Issues

#### 1. Celery Worker Not Processing Tasks

**Symptoms**: Workflows stuck in `PENDING` status

**Solutions**:
```bash
# Check worker logs
docker-compose logs celery-worker

# Verify Redis connection
docker-compose exec redis redis-cli ping

# Restart worker
docker-compose restart celery-worker
```

#### 2. Database Connection Errors

**Symptoms**: `SQLSTATE[08006] could not connect to server`

**Solutions**:
```bash
# Check PostgreSQL health
docker-compose ps postgres

# View database logs
docker-compose logs postgres

# Verify connection string in .env
echo $DATABASE_URL
```

#### 3. ChromaDB Embedding Failures

**Symptoms**: `Failed to embed document` errors

**Solutions**:
```bash
# Check ChromaDB service
curl http://localhost:8001/api/v1/heartbeat

# Verify ChromaDB logs
docker-compose logs chromadb

# Reset ChromaDB (WARNING: deletes all data)
docker-compose exec chromadb curl -X POST http://localhost:8000/api/v1/reset
```

#### 4. GitHub API Rate Limiting

**Symptoms**: `403 Forbidden` on GitHub issue creation

**Solutions**:
```bash
# Check rate limit status
curl -H "Authorization: token $GITHUB_TOKEN" \
  https://api.github.com/rate_limit

# Disable GitHub integration temporarily
export GITHUB_ENABLED=false
```

#### 5. KB Sync Lock Timeout

**Symptoms**: `KB sync workflow is already running` persists

**Solutions**:
```bash
# Check Redis lock
docker-compose exec redis redis-cli GET "lock:kb_sync"

# Manually release lock (emergency only)
docker-compose exec redis redis-cli DEL "lock:kb_sync"
```

### Debug Mode

Enable debug logging:

```env
# In .env
LOG_LEVEL=DEBUG
CELERY_LOG_LEVEL=DEBUG
```

### Logs Location

```bash
# View all logs
docker-compose logs -f

# Backend logs
docker-compose logs -f backend

# Worker logs
docker-compose logs -f celery-worker
```

---

## ğŸ¤ Contributing

Contributions are welcome! 




---

## ğŸ“ Support

For issues, questions, or contributions:

- **GitHub Issues**: https://github.com/YOUR_USERNAME/ia-dev-final-project/issues
- **Documentation**: See `/docs` directory for detailed guides


---

## ğŸ™ Acknowledgments


- **Celery** - Distributed task queue
- **Anthropic Claude** - AI-powered analysis and generation
- **ChromaDB** - Vector database for embeddings
- **DataTalks.Club** - AI & Dev Tools Bootcamp

